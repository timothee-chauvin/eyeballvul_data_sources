{"schema_version":"1.7.3","id":"CVE-2025-68756","published":"2026-01-05T09:32:29.824Z","modified":"2026-01-06T03:45:44.885724Z","summary":"block: Use RCU in blk_mq_[un]quiesce_tagset() instead of set->tag_list_lock","details":"In the Linux kernel, the following vulnerability has been resolved:\n\nblock: Use RCU in blk_mq_[un]quiesce_tagset() instead of set->tag_list_lock\n\nblk_mq_{add,del}_queue_tag_set() functions add and remove queues from\ntagset, the functions make sure that tagset and queues are marked as\nshared when two or more queues are attached to the same tagset.\nInitially a tagset starts as unshared and when the number of added\nqueues reaches two, blk_mq_add_queue_tag_set() marks it as shared along\nwith all the queues attached to it. When the number of attached queues\ndrops to 1 blk_mq_del_queue_tag_set() need to mark both the tagset and\nthe remaining queues as unshared.\n\nBoth functions need to freeze current queues in tagset before setting on\nunsetting BLK_MQ_F_TAG_QUEUE_SHARED flag. While doing so, both functions\nhold set->tag_list_lock mutex, which makes sense as we do not want\nqueues to be added or deleted in the process. This used to work fine\nuntil commit 98d81f0df70c (\"nvme: use blk_mq_[un]quiesce_tagset\")\nmade the nvme driver quiesce tagset instead of quiscing individual\nqueues. blk_mq_quiesce_tagset() does the job and quiesce the queues in\nset->tag_list while holding set->tag_list_lock also.\n\nThis results in deadlock between two threads with these stacktraces:\n\n  __schedule+0x47c/0xbb0\n  ? timerqueue_add+0x66/0xb0\n  schedule+0x1c/0xa0\n  schedule_preempt_disabled+0xa/0x10\n  __mutex_lock.constprop.0+0x271/0x600\n  blk_mq_quiesce_tagset+0x25/0xc0\n  nvme_dev_disable+0x9c/0x250\n  nvme_timeout+0x1fc/0x520\n  blk_mq_handle_expired+0x5c/0x90\n  bt_iter+0x7e/0x90\n  blk_mq_queue_tag_busy_iter+0x27e/0x550\n  ? __blk_mq_complete_request_remote+0x10/0x10\n  ? __blk_mq_complete_request_remote+0x10/0x10\n  ? __call_rcu_common.constprop.0+0x1c0/0x210\n  blk_mq_timeout_work+0x12d/0x170\n  process_one_work+0x12e/0x2d0\n  worker_thread+0x288/0x3a0\n  ? rescuer_thread+0x480/0x480\n  kthread+0xb8/0xe0\n  ? kthread_park+0x80/0x80\n  ret_from_fork+0x2d/0x50\n  ? kthread_park+0x80/0x80\n  ret_from_fork_asm+0x11/0x20\n\n  __schedule+0x47c/0xbb0\n  ? xas_find+0x161/0x1a0\n  schedule+0x1c/0xa0\n  blk_mq_freeze_queue_wait+0x3d/0x70\n  ? destroy_sched_domains_rcu+0x30/0x30\n  blk_mq_update_tag_set_shared+0x44/0x80\n  blk_mq_exit_queue+0x141/0x150\n  del_gendisk+0x25a/0x2d0\n  nvme_ns_remove+0xc9/0x170\n  nvme_remove_namespaces+0xc7/0x100\n  nvme_remove+0x62/0x150\n  pci_device_remove+0x23/0x60\n  device_release_driver_internal+0x159/0x200\n  unbind_store+0x99/0xa0\n  kernfs_fop_write_iter+0x112/0x1e0\n  vfs_write+0x2b1/0x3d0\n  ksys_write+0x4e/0xb0\n  do_syscall_64+0x5b/0x160\n  entry_SYSCALL_64_after_hwframe+0x4b/0x53\n\nThe top stacktrace is showing nvme_timeout() called to handle nvme\ncommand timeout. timeout handler is trying to disable the controller and\nas a first step, it needs to blk_mq_quiesce_tagset() to tell blk-mq not\nto call queue callback handlers. The thread is stuck waiting for\nset->tag_list_lock as it tries to walk the queues in set->tag_list.\n\nThe lock is held by the second thread in the bottom stack which is\nwaiting for one of queues to be frozen. The queue usage counter will\ndrop to zero after nvme_timeout() finishes, and this will not happen\nbecause the thread will wait for this mutex forever.\n\nGiven that [un]quiescing queue is an operation that does not need to\nsleep, update blk_mq_[un]quiesce_tagset() to use RCU instead of taking\nset->tag_list_lock, update blk_mq_{add,del}_queue_tag_set() to use RCU\nsafe list operations. Also, delete INIT_LIST_HEAD(&q->tag_set_list)\nin blk_mq_del_queue_tag_set() because we can not re-initialize it while\nthe list is being traversed under RCU. The deleted queue will not be\nadded/deleted to/from a tagset and it will be freed in blk_free_queue()\nafter the end of RCU grace period.","affected":[{"ranges":[{"type":"GIT","repo":"https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git","events":[{"introduced":"98d81f0df70ce6fc48517d938026e3c684b9051a"},{"fixed":"3baeec23a82e7ee9691f434c6ab0ab1387326108"},{"fixed":"6e8d363786765a81e35083e0909e076796468edf"},{"fixed":"ef0cd7b694928573f6569e61c14f5f059253162e"},{"fixed":"59e25ef2b413c72da6686d431e7759302cfccafa"}]}],"versions":["v6.1","v6.1-rc3","v6.1-rc4","v6.1-rc5","v6.1-rc6","v6.1-rc7","v6.1-rc8","v6.10","v6.10-rc1","v6.10-rc2","v6.10-rc3","v6.10-rc4","v6.10-rc5","v6.10-rc6","v6.10-rc7","v6.11","v6.11-rc1","v6.11-rc2","v6.11-rc3","v6.11-rc4","v6.11-rc5","v6.11-rc6","v6.11-rc7","v6.12","v6.12-rc1","v6.12-rc2","v6.12-rc3","v6.12-rc4","v6.12-rc5","v6.12-rc6","v6.12-rc7","v6.12.1","v6.12.10","v6.12.11","v6.12.12","v6.12.13","v6.12.14","v6.12.15","v6.12.16","v6.12.17","v6.12.18","v6.12.19","v6.12.2","v6.12.20","v6.12.21","v6.12.22","v6.12.23","v6.12.24","v6.12.25","v6.12.26","v6.12.27","v6.12.28","v6.12.29","v6.12.3","v6.12.30","v6.12.31","v6.12.32","v6.12.33","v6.12.34","v6.12.35","v6.12.36","v6.12.37","v6.12.38","v6.12.39","v6.12.4","v6.12.40","v6.12.41","v6.12.42","v6.12.43","v6.12.44","v6.12.45","v6.12.46","v6.12.47","v6.12.48","v6.12.49","v6.12.5","v6.12.50","v6.12.51","v6.12.52","v6.12.53","v6.12.54","v6.12.55","v6.12.56","v6.12.57","v6.12.58","v6.12.59","v6.12.6","v6.12.60","v6.12.61","v6.12.62","v6.12.7","v6.12.8","v6.12.9","v6.13","v6.13-rc1","v6.13-rc2","v6.13-rc3","v6.13-rc4","v6.13-rc5","v6.13-rc6","v6.13-rc7","v6.14","v6.14-rc1","v6.14-rc2","v6.14-rc3","v6.14-rc4","v6.14-rc5","v6.14-rc6","v6.14-rc7","v6.15","v6.15-rc1","v6.15-rc2","v6.15-rc3","v6.15-rc4","v6.15-rc5","v6.15-rc6","v6.15-rc7","v6.16","v6.16-rc1","v6.16-rc2","v6.16-rc3","v6.16-rc4","v6.16-rc5","v6.16-rc6","v6.16-rc7","v6.17","v6.17-rc1","v6.17-rc2","v6.17-rc3","v6.17-rc4","v6.17-rc5","v6.17-rc6","v6.17-rc7","v6.17.1","v6.17.10","v6.17.11","v6.17.12","v6.17.2","v6.17.3","v6.17.4","v6.17.5","v6.17.6","v6.17.7","v6.17.8","v6.17.9","v6.18","v6.18-rc1","v6.18-rc2","v6.18-rc3","v6.18-rc4","v6.18-rc5","v6.18-rc6","v6.18-rc7","v6.18.1","v6.2","v6.2-rc1","v6.2-rc2","v6.2-rc3","v6.2-rc4","v6.2-rc5","v6.2-rc6","v6.2-rc7","v6.2-rc8","v6.3","v6.3-rc1","v6.3-rc2","v6.3-rc3","v6.3-rc4","v6.3-rc5","v6.3-rc6","v6.3-rc7","v6.4","v6.4-rc1","v6.4-rc2","v6.4-rc3","v6.4-rc4","v6.4-rc5","v6.4-rc6","v6.4-rc7","v6.5","v6.5-rc1","v6.5-rc2","v6.5-rc3","v6.5-rc4","v6.5-rc5","v6.5-rc6","v6.5-rc7","v6.6","v6.6-rc1","v6.6-rc2","v6.6-rc3","v6.6-rc4","v6.6-rc5","v6.6-rc6","v6.6-rc7","v6.7","v6.7-rc1","v6.7-rc2","v6.7-rc3","v6.7-rc4","v6.7-rc5","v6.7-rc6","v6.7-rc7","v6.7-rc8","v6.8","v6.8-rc1","v6.8-rc2","v6.8-rc3","v6.8-rc4","v6.8-rc5","v6.8-rc6","v6.8-rc7","v6.9","v6.9-rc1","v6.9-rc2","v6.9-rc3","v6.9-rc4","v6.9-rc5","v6.9-rc6","v6.9-rc7"],"database_specific":{"source":"https://storage.googleapis.com/cve-osv-conversion/osv-output/CVE-2025-68756.json"}},{"package":{"name":"Kernel","ecosystem":"Linux"},"ranges":[{"type":"ECOSYSTEM","events":[{"introduced":"6.2.0"},{"fixed":"6.12.63"}]},{"type":"ECOSYSTEM","events":[{"introduced":"6.13.0"},{"fixed":"6.17.13"}]},{"type":"ECOSYSTEM","events":[{"introduced":"6.18.0"},{"fixed":"6.18.2"}]}],"database_specific":{"source":"https://storage.googleapis.com/cve-osv-conversion/osv-output/CVE-2025-68756.json"}}],"references":[{"type":"PACKAGE","url":"https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git"},{"type":"WEB","url":"https://git.kernel.org/stable/c/3baeec23a82e7ee9691f434c6ab0ab1387326108"},{"type":"WEB","url":"https://git.kernel.org/stable/c/59e25ef2b413c72da6686d431e7759302cfccafa"},{"type":"WEB","url":"https://git.kernel.org/stable/c/6e8d363786765a81e35083e0909e076796468edf"},{"type":"WEB","url":"https://git.kernel.org/stable/c/ef0cd7b694928573f6569e61c14f5f059253162e"},{"type":"ADVISORY","url":"https://github.com/CVEProject/cvelistV5/tree/main/cves/2025/68xxx/CVE-2025-68756.json"},{"type":"ADVISORY","url":"https://nvd.nist.gov/vuln/detail/CVE-2025-68756"}],"database_specific":{"cna_assigner":"Linux","osv_generated_from":"https://github.com/CVEProject/cvelistV5/tree/main/cves/2025/68xxx/CVE-2025-68756.json"}}